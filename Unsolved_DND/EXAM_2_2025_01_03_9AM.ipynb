{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p415H870SFoQ"
      },
      "source": [
        "# **Final Exam - Deep Network Development**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhzDq8zcLozF"
      },
      "source": [
        "# **Exam Information**\n",
        "\n",
        "---\n",
        "\n",
        "- **Name:** Mir Mohibullah Sazid\n",
        "- **Neptun ID:** GVFPV8\n",
        "- **Date:** *03/01/2025*\n",
        "- **Duration:** *9:00 AM ‚Äì 11:00 AM*\n",
        "- *Please fill in your details above before starting the exam.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBpfD9z3SiMV"
      },
      "source": [
        "## **General Rules**\n",
        "\n",
        "This notebook contains the task to be completed in order to pass the exam and the course. Below are the details:\n",
        "1. **Implementing a network architecture**, including its **forward pass** function.\n",
        "2. Additional **optional requirements** for bonus points towards final grade.\n",
        "3. You have **2 hours** to complete the exam.\n",
        "4. You may distribute the time as you see fit between the required and optional parts.\n",
        "5. You are allowed to use any resource including: the internet, AI tools, practice notebooks, and more.\n",
        "6. **It is strictly prohibited to use any form of communication** (e.g., Teams, WhatsApp, Messenger, etc.). **Violation will result in an immediate FAIL** of the exam.\n",
        "\n",
        "---\n",
        "\n",
        "### **Submission Guidelines**\n",
        "- Submit your solution as a **`.ipynb` file** on **Canvas**.\n",
        "- To **PASS**, your solution must:\n",
        "  1. **Satisfy the minimum requirements** (i.e., a working implementation of the network architecture and forward pass).\n",
        "  2. Be **submitted on time**.\n",
        "  3. Be prepared to **orally defend your code** after submission.\n",
        "\n",
        "---\n",
        "\n",
        "### **Exam Retake Policy**\n",
        "- If you **FAIL**, you are allowed to do **one retake**.  \n",
        "- If you **FAIL AGAIN**, sadly, you **fail the course**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Grading**\n",
        "- If you **PASS**, your final grade will be the **weighted average** of your assignment defenses (theory and code).\n",
        "\n",
        "---\n",
        "\n",
        "Good luck, and ensure you follow all the rules!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eJ8a7vIUKLh"
      },
      "source": [
        "## **Requirements**\n",
        "\n",
        "---\n",
        "\n",
        "### **Minimum Requirements ‚Äì Sufficient to Pass the Exam**\n",
        "1. **Implement the layers of the architecture:**  \n",
        "   Complete the architecture outlined in Section 1 by filling in the missing parts.\n",
        "2. **Implement the forward function:**  \n",
        "   Ensure the input and output of the forward function are correctly implemented.  \n",
        "   \n",
        "   **Note:** To meet these requirements, your final output must match the expected output.\n",
        "\n",
        "---\n",
        "\n",
        "### **Extra Requirements ‚Äì For Grade Improvement and AI Lab Access**\n",
        "\n",
        "---\n",
        "\n",
        "3. **Text-to-Image with Image-Guided Embeddings:**  \n",
        "- The goal is to perform text-to-image generation using an existing image as a guide for editing. The input text specifies modifications to the existing image, preserving its content while applying specific changes as described by the text.\n",
        "\n",
        "   ‚û°Ô∏è **Reward: +1 to final grade**\n",
        "\n",
        "---\n",
        "\n",
        "4. **Replacing Text Encoder with Transformer:**  \n",
        "- Replace the text encoder with a Transformer model.\n",
        "- Test the new architecture to ensure it performs text-to-image editing correctly, by satisfying the expect output condition.\n",
        "\n",
        "   ‚û°Ô∏è **Reward: Access to AI Lab**\n",
        "\n",
        "---\n",
        "\n",
        "Make sure to carefully follow the instructions provided in each cell to meet the requirements!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeb5t4EiSld1"
      },
      "source": [
        "## **1. Required: Task Description**\n",
        "\n",
        "Your task is to implement a custom neural network architecture along with its forward pass function.\n",
        "\n",
        "This task is inspired by **text-to-image generation**, where a neural network maps a sequence of tokens representing textual information into a high-dimensional image. The text input is typically **tokenized** into a sequence of integers. This representation can be passed through an **encoder-decoder** architecture to generate images.\n",
        "\n",
        "For this task, you will work with a simplified text-to-image representation in the form of a random tensor with the shape **(1, 10)**:\n",
        "- The 1 indicates that there is a single input sample.\n",
        "- 10 corresponds to the sequence length of the input text tokens.\n",
        "\n",
        "Your implemented model will:\n",
        "- Take this text token tensor as input.\n",
        "- Encode it into a latent representation.\n",
        "- Decode the latent representation to produce an output **image tensor of shape (1, 3, 256, 256)**, where:\n",
        "    - 1 represents the batch size.\n",
        "    - 3 indicates the RGB color channels of the image.\n",
        "    - 256 √ó 256 corresponds to the height and width of the output image.\n",
        "\n",
        "The primary objective is to correctly implement the neural network architecture and its forward pass to achieve the desired functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV5xi065Fk1x"
      },
      "source": [
        "To better view the architecture diagram:  \n",
        "- **Right-click the image** and select **\"Open image in a new tab\"** to enable zoom for a clearer view.  \n",
        "- Alternatively, you can **download the image** using the link below:  \n",
        "  [Download Architecture Diagram](https://drive.google.com/file/d/1osVNVcsNGo-d9DCGVH1hJDw2nw4rMToR/view?usp=sharing)\n",
        "\n",
        "---\n",
        "\n",
        "### Diagram Preview:\n",
        "![Architecture Diagram](https://drive.google.com/uc?export=view&id=1osVNVcsNGo-d9DCGVH1hJDw2nw4rMToR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFHVdn0GF95L"
      },
      "source": [
        "Necessary Imports and Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FAofci_7R6kB"
      },
      "outputs": [],
      "source": [
        "# Cell 0.1\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgOK-xATFdDW",
        "outputId": "10a9412f-adf1-4db0-8540-24d319927cc7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cell 0.2 (GPU is not needed)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M35QxrIgfug8"
      },
      "source": [
        "### Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVP_ivSyOfkQ",
        "outputId": "0cf77c26-dca2-4862-d5a6-c8af1bc95fdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 10])\n"
          ]
        }
      ],
      "source": [
        "# Cell 0.3 -> INPUT (DO NOT EDIT THIS CELL!)\n",
        "vocab_size = 10\n",
        "input_tokens = torch.randint(0, vocab_size, (1, 10))\n",
        "print(input_tokens.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99eJ_wATFRv7"
      },
      "source": [
        "### Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text input: torch.Size([1, 10])\n",
            "After embedding: torch.Size([1, 10, 256])\n",
            "After LSTM: torch.Size([1, 10, 128])\n",
            "Last hidden state: torch.Size([1, 128])\n",
            "After linear layer: torch.Size([1, 10240])\n",
            "After reshape: torch.Size([1, 10, 32, 32])\n",
            "Model output shape: torch.Size([1, 10, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(TextEncoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_size = 128\n",
        "        self.num_layers = 2\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, \n",
        "                            hidden_size=self.hidden_size, \n",
        "                            num_layers=self.num_layers, \n",
        "                            batch_first=True)\n",
        "\n",
        "        # Linear layer\n",
        "        self.linear = nn.Linear(self.hidden_size, 10 * 32 * 32)\n",
        "\n",
        "    def forward(self, input_tokens):\n",
        "        print(\"Text input:\", input_tokens.shape)\n",
        "\n",
        "        # Embedding\n",
        "        embedded = self.embedding(input_tokens)\n",
        "        print(\"After embedding:\", embedded.shape)\n",
        "\n",
        "        # LSTM\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        print(\"After LSTM:\", lstm_out.shape)\n",
        "\n",
        "        # Extract the output of the last time step\n",
        "        last_hidden_state = lstm_out[:, -1, :]\n",
        "        print(\"Last hidden state:\", last_hidden_state.shape)\n",
        "\n",
        "        # Linear transformation\n",
        "        linear_out = self.linear(last_hidden_state)\n",
        "        print(\"After linear layer:\", linear_out.shape)\n",
        "\n",
        "        # Reshape to (1, 10, 32, 32)\n",
        "        out = linear_out.view(-1, 10, 32, 32)\n",
        "        print(\"After reshape:\", out.shape)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Initialize and test the model\n",
        "vocab_size = 10\n",
        "embedding_dim = 256\n",
        "input_tokens = torch.randint(0, vocab_size, (1, 10))\n",
        "\n",
        "model = TextEncoder(vocab_size, embedding_dim)\n",
        "output = model(input_tokens)\n",
        "print(\"Model output shape:\", output.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text embedding: torch.Size([1, 10, 32, 32])\n",
            "After first ConvTranspose2D: torch.Size([1, 16, 136, 136])\n",
            "After first Conv2D: torch.Size([1, 32, 30, 30])\n",
            "After MaxPool2D: torch.Size([1, 32, 15, 15])\n",
            "After ReLU: torch.Size([1, 32, 15, 15])\n",
            "After BatchNorm2D: torch.Size([1, 32, 15, 15])\n",
            "After MaxPool2D: torch.Size([1, 32, 7, 7])\n",
            "After second Conv2D: torch.Size([1, 32, 28, 28])\n",
            "After ReLU: torch.Size([1, 32, 28, 28])\n",
            "After BatchNorm2D: torch.Size([1, 32, 28, 28])\n",
            "After third Conv2D: torch.Size([1, 64, 26, 26])\n",
            "After second ConvTranspose2D: torch.Size([1, 32, 28, 28])\n",
            "After add operation: torch.Size([1, 32, 28, 28])\n",
            "After add operation: torch.Size([1, 32, 7, 7])\n",
            "After add operation: torch.Size([1, 32, 7, 7])\n",
            "After final ConvTranspose2D: torch.Size([1, 16, 136, 136])\n",
            "After concatenation: torch.Size([1, 32, 136, 136])\n",
            "After final ConvTranspose2D: torch.Size([1, 8, 271, 271])\n",
            "After final Conv2D: torch.Size([1, 3, 256, 256])\n",
            "Output image shape: torch.Size([1, 3, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ImageDecoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageDecoder, self).__init__()\n",
        "        \n",
        "        # First ConvTranspose2D layer\n",
        "        self.conv_trans1 = nn.ConvTranspose2d(in_channels=10, out_channels=16, kernel_size=12,stride=4,padding=0)\n",
        "        \n",
        "        \n",
        "        \n",
        "        # Conv2D + ReLU\n",
        "        self.conv1 = nn.Conv2d(in_channels=10, out_channels=32, kernel_size=3, stride=1, padding=0)\n",
        "        \n",
        "        \n",
        "        # MaxPool2D\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        # BatchNorm2D\n",
        "        self.batch_norm1 = nn.BatchNorm2d(num_features=32)\n",
        "\n",
        "        self.max_pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        \n",
        "        # Second Conv2D + ReLU\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=0)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        self.batch_norm2 = nn.BatchNorm2d(num_features=32)\n",
        "        \n",
        "        # Conv2D (adds residual connection)\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0)\n",
        "        \n",
        "        # ConvTranspose2D\n",
        "        self.conv_trans2 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=0)\n",
        "        \n",
        "        # Add operation (for skip connection)\n",
        "        self.add1 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=8, stride=3, padding=0)\n",
        "        \n",
        "        # Final ConvTranspose2D layers\n",
        "        self.final_conv_trans = nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=24, stride=19, padding=1)\n",
        "        self.final_conv_trans2 = nn.ConvTranspose2d(in_channels=32, out_channels=8, kernel_size=3, stride=2, padding=1)\n",
        "        self.final_conv = nn.Conv2d(in_channels=8, out_channels=3, kernel_size=16, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, text_embedding):\n",
        "        print(\"Text embedding:\", text_embedding.shape)\n",
        "        \n",
        "        # ConvTranspose2D\n",
        "        x1 = self.conv_trans1(text_embedding)\n",
        "        print(\"After first ConvTranspose2D:\", x1.shape)\n",
        "\n",
        "        x2 = self.conv1(text_embedding)\n",
        "        print(\"After first Conv2D:\", x2.shape)\n",
        "        x = self.max_pool(x2)\n",
        "        print(\"After MaxPool2D:\", x.shape)\n",
        "        x = self.relu1(x)\n",
        "        print(\"After ReLU:\", x.shape)\n",
        "        x = self.batch_norm1(x)\n",
        "        print(\"After BatchNorm2D:\", x.shape)\n",
        "        y5 = self.max_pool1(x)\n",
        "        print(\"After MaxPool2D:\", y5.shape)\n",
        "\n",
        "        x3 = self.conv2(x2)\n",
        "        print(\"After second Conv2D:\", x3.shape)\n",
        "        x = self.relu2(x3)\n",
        "        print(\"After ReLU:\", x.shape)\n",
        "        y = self.batch_norm2(x)\n",
        "        print(\"After BatchNorm2D:\", y.shape)\n",
        "\n",
        "        x4 = self.conv3(x3)\n",
        "        print(\"After third Conv2D:\", x4.shape)\n",
        "        y1 = self.conv_trans2(x4)\n",
        "        print(\"After second ConvTranspose2D:\", y1.shape)\n",
        "\n",
        "        y3 = y1 + y\n",
        "        print(\"After add operation:\", y3.shape)\n",
        "        y4 = self.add1(y3)\n",
        "        print(\"After add operation:\", y4.shape)\n",
        "        y6 = y4 + y5\n",
        "        print(\"After add operation:\", y6.shape)\n",
        "\n",
        "        # Final ConvTranspose2D layers\n",
        "        x = self.final_conv_trans(y6)\n",
        "        print(\"After final ConvTranspose2D:\", x.shape)\n",
        "\n",
        "        x = torch.concat((x, x1), 1)\n",
        "        print(\"After concatenation:\", x.shape)\n",
        "\n",
        "        x = self.final_conv_trans2(x)\n",
        "        print(\"After final ConvTranspose2D:\", x.shape)\n",
        "\n",
        "        x = self.final_conv(x)\n",
        "        print(\"After final Conv2D:\", x.shape)\n",
        "        \n",
        "      \n",
        "        \n",
        "        return x\n",
        "\n",
        "# Test the ImageDecoder with a dummy input\n",
        "dummy_input = torch.randn(1, 10, 32, 32)  # Text embedding output\n",
        "decoder = ImageDecoder()\n",
        "output_image = decoder(dummy_input)\n",
        "print(\"Output image shape:\", output_image.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JqE_d_vzQZmH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(TextEncoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_size = 128\n",
        "        self.num_layers = 2\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, \n",
        "                            hidden_size=self.hidden_size, \n",
        "                            num_layers=self.num_layers, \n",
        "                            batch_first=True)\n",
        "\n",
        "        # Linear layer\n",
        "        self.linear = nn.Linear(self.hidden_size, 10 * 32 * 32)\n",
        "\n",
        "    def forward(self, input_tokens):\n",
        "        print(\"Text input:\", input_tokens.shape)\n",
        "\n",
        "        # Embedding\n",
        "        embedded = self.embedding(input_tokens)\n",
        "        print(\"After embedding:\", embedded.shape)\n",
        "\n",
        "        # LSTM\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        print(\"After LSTM:\", lstm_out.shape)\n",
        "\n",
        "        # Extract the output of the last time step\n",
        "        last_hidden_state = lstm_out[:, -1, :]\n",
        "        print(\"Last hidden state:\", last_hidden_state.shape)\n",
        "\n",
        "        # Linear transformation\n",
        "        linear_out = self.linear(last_hidden_state)\n",
        "        print(\"After linear layer:\", linear_out.shape)\n",
        "\n",
        "        # Reshape to (1, 10, 32, 32)\n",
        "        out = linear_out.view(-1, 10, 32, 32)\n",
        "        print(\"After reshape:\", out.shape)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "z1ilsWmEHY-8"
      },
      "outputs": [],
      "source": [
        "class ImageDecoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageDecoder, self).__init__()\n",
        "        \n",
        "        # First ConvTranspose2D layer\n",
        "        self.conv_trans1 = nn.ConvTranspose2d(in_channels=10, out_channels=16, kernel_size=12,stride=4,padding=0)\n",
        "        \n",
        "        \n",
        "        \n",
        "        # Conv2D + ReLU\n",
        "        self.conv1 = nn.Conv2d(in_channels=10, out_channels=32, kernel_size=3, stride=1, padding=0)\n",
        "        \n",
        "        \n",
        "        # MaxPool2D\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        # BatchNorm2D\n",
        "        self.batch_norm1 = nn.BatchNorm2d(num_features=32)\n",
        "\n",
        "        self.max_pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        \n",
        "        # Second Conv2D + ReLU\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=0)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        self.batch_norm2 = nn.BatchNorm2d(num_features=32)\n",
        "        \n",
        "        # Conv2D (adds residual connection)\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0)\n",
        "        \n",
        "        # ConvTranspose2D\n",
        "        self.conv_trans2 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=0)\n",
        "        \n",
        "        # Add operation (for skip connection)\n",
        "        self.add1 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=8, stride=3, padding=0)\n",
        "        \n",
        "        # Final ConvTranspose2D layers\n",
        "        self.final_conv_trans = nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=24, stride=19, padding=1)\n",
        "        self.final_conv_trans2 = nn.ConvTranspose2d(in_channels=32, out_channels=8, kernel_size=3, stride=2, padding=1)\n",
        "        self.final_conv = nn.Conv2d(in_channels=8, out_channels=3, kernel_size=16, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, text_embedding):\n",
        "        print(\"Text embedding:\", text_embedding.shape)\n",
        "        \n",
        "        # ConvTranspose2D\n",
        "        x1 = self.conv_trans1(text_embedding)\n",
        "        print(\"After first ConvTranspose2D:\", x1.shape)\n",
        "\n",
        "        x2 = self.conv1(text_embedding)\n",
        "        print(\"After first Conv2D:\", x2.shape)\n",
        "        x = self.max_pool(x2)\n",
        "        print(\"After MaxPool2D:\", x.shape)\n",
        "        x = self.relu1(x)\n",
        "        print(\"After ReLU:\", x.shape)\n",
        "        x = self.batch_norm1(x)\n",
        "        print(\"After BatchNorm2D:\", x.shape)\n",
        "        y5 = self.max_pool1(x)\n",
        "        print(\"After MaxPool2D:\", y5.shape)\n",
        "\n",
        "        x3 = self.conv2(x2)\n",
        "        print(\"After second Conv2D:\", x3.shape)\n",
        "        x = self.relu2(x3)\n",
        "        print(\"After ReLU:\", x.shape)\n",
        "        y = self.batch_norm2(x)\n",
        "        print(\"After BatchNorm2D:\", y.shape)\n",
        "\n",
        "        x4 = self.conv3(x3)\n",
        "        print(\"After third Conv2D:\", x4.shape)\n",
        "        y1 = self.conv_trans2(x4)\n",
        "        print(\"After second ConvTranspose2D:\", y1.shape)\n",
        "\n",
        "        y3 = y1 + y\n",
        "        print(\"After add operation:\", y3.shape)\n",
        "        y4 = self.add1(y3)\n",
        "        print(\"After add operation:\", y4.shape)\n",
        "        y6 = y4 + y5\n",
        "        print(\"After add operation:\", y6.shape)\n",
        "\n",
        "        # Final ConvTranspose2D layers\n",
        "        x = self.final_conv_trans(y6)\n",
        "        print(\"After final ConvTranspose2D:\", x.shape)\n",
        "\n",
        "        x = torch.concat((x, x1), 1)\n",
        "        print(\"After concatenation:\", x.shape)\n",
        "\n",
        "        x = self.final_conv_trans2(x)\n",
        "        print(\"After final ConvTranspose2D:\", x.shape)\n",
        "\n",
        "        x = self.final_conv(x)\n",
        "        print(\"After final Conv2D:\", x.shape)\n",
        "        \n",
        "      \n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENO4kK0X6N5S"
      },
      "source": [
        "#### Test your implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "um3Aa5zXIqAt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoder:\n",
            "Text input: torch.Size([1, 10])\n",
            "After embedding: torch.Size([1, 10, 256])\n",
            "After LSTM: torch.Size([1, 10, 128])\n",
            "Last hidden state: torch.Size([1, 128])\n",
            "After linear layer: torch.Size([1, 10240])\n",
            "After reshape: torch.Size([1, 10, 32, 32])\n",
            "Decoder:\n",
            "Text embedding: torch.Size([1, 10, 32, 32])\n",
            "After first ConvTranspose2D: torch.Size([1, 16, 136, 136])\n",
            "After first Conv2D: torch.Size([1, 32, 30, 30])\n",
            "After MaxPool2D: torch.Size([1, 32, 15, 15])\n",
            "After ReLU: torch.Size([1, 32, 15, 15])\n",
            "After BatchNorm2D: torch.Size([1, 32, 15, 15])\n",
            "After MaxPool2D: torch.Size([1, 32, 7, 7])\n",
            "After second Conv2D: torch.Size([1, 32, 28, 28])\n",
            "After ReLU: torch.Size([1, 32, 28, 28])\n",
            "After BatchNorm2D: torch.Size([1, 32, 28, 28])\n",
            "After third Conv2D: torch.Size([1, 64, 26, 26])\n",
            "After second ConvTranspose2D: torch.Size([1, 32, 28, 28])\n",
            "After add operation: torch.Size([1, 32, 28, 28])\n",
            "After add operation: torch.Size([1, 32, 7, 7])\n",
            "After add operation: torch.Size([1, 32, 7, 7])\n",
            "After final ConvTranspose2D: torch.Size([1, 16, 136, 136])\n",
            "After concatenation: torch.Size([1, 32, 136, 136])\n",
            "After final ConvTranspose2D: torch.Size([1, 8, 271, 271])\n",
            "After final Conv2D: torch.Size([1, 3, 256, 256])\n",
            "\n",
            "üéâ Congratulations! Your implementation is correct. You passed the minimum requirement! üéâ\n"
          ]
        }
      ],
      "source": [
        "#DO NOT MODIFY THIS CELL\n",
        "\n",
        "embedding_dim = 256\n",
        "print(\"Encoder:\")\n",
        "text_encoder = TextEncoder(vocab_size, embedding_dim)\n",
        "text_embedding = text_encoder(input_tokens)\n",
        "print(\"Decoder:\")\n",
        "image_decoder = ImageDecoder()\n",
        "image = image_decoder(text_embedding)\n",
        "\n",
        "try:\n",
        "    assert text_embedding.shape == (1, 10, 32, 32), \"Encoded text shape is incorrect.\"\n",
        "    assert image.shape == (1, 3, 256, 256), \"Decoded image shape is incorrect.\"\n",
        "    print(\"\\nüéâ Congratulations! Your implementation is correct. You passed the minimum requirement! üéâ\")\n",
        "except AssertionError as e:\n",
        "    print(f\"\\n‚ùå Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNJYNiAMYSwQ"
      },
      "source": [
        "## **2. Optional: +1 to the Final Grade**\n",
        "- Add another input tensor: a random tensor of size (1, 3, 256, 256).\n",
        "- Implement an Image Encoder with a few layers to encode the tensor. The encoding process should follow the example image provided.\n",
        "- Combine the encoded image embeddings with the text embeddings using cross-attention, following the example image provided.\n",
        "\n",
        "You should only add these new parts and reuse the ImageDecoder previously created. The final output should still be the same as in the previously required task (1,3,256,256)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5hswDE_GmH4"
      },
      "source": [
        "To better view the architecture diagram:  \n",
        "- **Right-click the image** and select **\"Open image in a new tab\"** to enable zoom for a clearer view.  \n",
        "- Alternatively, you can **download the image** using the link below:  \n",
        "  [Download Architecture Diagram](https://drive.google.com/file/d/1nIgqhyPq0eKWEvT7leqoa0ZeBApCCs6u/view?usp=sharing)\n",
        "\n",
        "---\n",
        "\n",
        "### Diagram Preview:\n",
        "![Architecture Diagram](https://drive.google.com/uc?export=view&id=1nIgqhyPq0eKWEvT7leqoa0ZeBApCCs6u)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U55r4Zw74Jgn"
      },
      "source": [
        "#### New Input - create a random tensor of size (1,3,256,256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8mmiPG-4ax-"
      },
      "outputs": [],
      "source": [
        "# ADD YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abMcDR7t4mXE"
      },
      "source": [
        "#### Image Encoder - create the image encoder, following the example provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7upryOE4ZKE"
      },
      "outputs": [],
      "source": [
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageEncoder, self).__init__()\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "    def forward(self, new_image):\n",
        "        print(\"New image:\", new_image.shape)\n",
        "        # ADD YOUR CODE HERE\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_nxf1x24qla"
      },
      "source": [
        "#### Combine with Cross-Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjQTQIYp4wn2"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, embed_dim=1024):\n",
        "        super(CrossAttention, self).__init__()\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "    def forward(self, text_embedding, image_embedding):\n",
        "        # ADD YOUR CODE HERE\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDA0I3Bb6Kis"
      },
      "source": [
        "#### Test your implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-xa3jT26S1U"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "image_encoder = ImageEncoder()\n",
        "image_embedding = image_encoder(new_image)\n",
        "\n",
        "embed_dim = 1024\n",
        "cross_attention = CrossAttention(embed_dim=embed_dim)\n",
        "combine = cross_attention(text_embedding, image_embedding)\n",
        "\n",
        "image = image_decoder(combine)\n",
        "\n",
        "try:\n",
        "    assert image_embedding.shape == (1, 10, 32, 32), \"Encoded image shape is incorrect.\"\n",
        "    assert combine.shape == (1, 10, 32, 32), \"Combined cross attention shape is incorrect.\"\n",
        "    assert image.shape == (1, 3, 256, 256), \"Combined cross attention shape is incorrect.\"\n",
        "    print(\"\\nüéâ Congratulations! Your implementation is correct. You increased your final grade by 1! üéâ\")\n",
        "except AssertionError as e:\n",
        "    print(f\"\\n‚ùå Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "637zP7iiaagl"
      },
      "source": [
        "## **3. Optional: Access to AI Lab**\n",
        "- Replace the text encoder with a Transformer model. This involves:\n",
        "    - Maximizing the sequence length to 16.\n",
        "    - Using BertTokenizer.\n",
        "    - Adding Positional Encoding.\n",
        "    - Defining a Transformer Encoder.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrIb6M38en7m"
      },
      "source": [
        "To better view the architecture diagram:  \n",
        "- **Right-click the image** and select **\"Open image in a new tab\"** to enable zoom for a clearer view.  \n",
        "- Alternatively, you can **download the image** using the link below:  \n",
        "  [Download Architecture Diagram](https://drive.google.com/file/d/1bwrPryFGAAFF3OoJ3Z7UUzpVJaYvijUg/view?usp=sharing)\n",
        "\n",
        "---\n",
        "\n",
        "### Diagram Preview:\n",
        "![Architecture Diagram](https://drive.google.com/uc?export=view&id=1bwrPryFGAAFF3OoJ3Z7UUzpVJaYvijUg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69ONfR0xRdE2"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=16):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lG655fyIen7n"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Transformer Encoder\n",
        "class TransformerTextEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=16, num_heads=4, num_layers=2):\n",
        "        super(TransformerTextEncoder, self).__init__()\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "    def forward(self, input_text):\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oSF35-ven7n"
      },
      "outputs": [],
      "source": [
        "\n",
        "#DO NOT MODIFY THIS CELL\n",
        "new_image = torch.randn((1,3,256,256))\n",
        "embedding_dim = 1024 # 32x32\n",
        "print(\"Encoder:\")\n",
        "text_encoder = TransformerTextEncoder(embedding_dim)\n",
        "input_text = [\"Generate an image based on this text\"]\n",
        "text_embedding = text_encoder(input_text)\n",
        "\n",
        "image_encoder = ImageEncoder()\n",
        "image_embedding = image_encoder(new_image)\n",
        "\n",
        "embed_dim = 1024\n",
        "cross_attention = CrossAttention(embed_dim=embed_dim)\n",
        "combine = cross_attention(text_embedding, image_embedding)\n",
        "\n",
        "print(\"Decoder:\")\n",
        "image_decoder = ImageDecoder()\n",
        "image = image_decoder(combine)\n",
        "\n",
        "try:\n",
        "    assert text_embedding.shape == (1, 16, 32, 32), \"Encoded text shape is incorrect.\"\n",
        "    assert image_embedding.shape == (1, 16, 32, 32), \"Encoded image shape is incorrect.\"\n",
        "    assert combine.shape == (1, 16, 32, 32), \"Combined cross attention shape is incorrect.\"\n",
        "    assert image.shape == (1, 3, 256, 256), \"Decoded image shape is incorrect.\"\n",
        "    print(\"\\nüéâ Congratulations! Your implementation is correct. You passed the requirement for the AI Lab! üéâ\")\n",
        "except AssertionError as e:\n",
        "    print(f\"\\n‚ùå Error: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
