{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Final Exam - Deep Network Development**\n",
        "\n"
      ],
      "metadata": {
        "id": "p415H870SFoQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exam Information**\n",
        "\n",
        "---\n",
        "\n",
        "- **Name:** *<Enter your name here>*\n",
        "- **Neptun ID:** *<Enter your Neptun ID here>*\n",
        "- **Date:** *18/12/2024*\n",
        "- **Duration:** *9:00 AM ‚Äì 11:00 AM*\n",
        "- **Instructions:** *Please fill in your details above before starting the exam. Ensure you adhere to the allotted time.*\n",
        "\n"
      ],
      "metadata": {
        "id": "IhzDq8zcLozF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **General Rules**\n",
        "\n",
        "This notebook contains the task to be completed in order to pass the exam and the course. The task consists of:\n",
        "1. **Implementing a network architecture**, including its **forward pass** function.\n",
        "2. Additional **optional requirements** for bonus points towards final grade.\n",
        "\n",
        "---\n",
        "\n",
        "### **Exam Duration**\n",
        "- You have **2 hours** to complete the exam.\n",
        "- You may distribute the time as you see fit between the required and optional parts.\n",
        "\n",
        "---\n",
        "\n",
        "### **Allowed and Prohibited Resources**\n",
        "- **Allowed**:\n",
        "  - The **internet**.\n",
        "  - **AI tools**.\n",
        "  - **Practice notebooks**.\n",
        "- **Prohibited**:\n",
        "  - Any form of **communication** (e.g., Teams, WhatsApp, Messenger, etc.).  \n",
        "  - **Violation** will result in an immediate **FAIL** of the exam.\n",
        "\n",
        "---\n",
        "\n",
        "### **Submission Guidelines**\n",
        "- Submit your solution as a **`.ipynb` file** on **Canvas**.\n",
        "- To **PASS**, your solution must:\n",
        "  1. **Satisfy the minimum requirements** (i.e., a working implementation of the network architecture and forward pass).\n",
        "  2. Be **submitted on time**.\n",
        "  3. Be prepared to **orally defend your code** after submission.\n",
        "\n",
        "---\n",
        "\n",
        "### **Exam Retake Policy**\n",
        "- If you **FAIL**, you are allowed **one retry**.  \n",
        "- If you **FAIL AGAIN**, you will have **failed the course**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Grading**\n",
        "- If you **PASS**, your final grade will be the **weighted average** of your assignment defenses (theory and code).\n",
        "\n",
        "---\n",
        "\n",
        "Good luck, and ensure you follow all the rules!\n"
      ],
      "metadata": {
        "id": "DBpfD9z3SiMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task Description**\n",
        "\n",
        "Your task is to implement a custom neural network architecture along with its forward pass function.\n",
        "\n",
        "This task is inspired by **spectrogram processing**, where a spectrogram represents the frequency content of an audio signal over time. Spectrograms are typically generated using transformations such as the **Wavelet Transform** or the **Fourier Transform**, resulting in a matrix representation. This representation can often be reshaped into a format such as **(2, X, Y)**, where:\n",
        "- **2** represents the stereo audio channels (left and right).\n",
        "- **X** corresponds to the frequency bins, capturing the range of frequencies in the audio signal.\n",
        "- **Y** represents the time intervals, showing how frequencies evolve over time.\n",
        "\n",
        "For this task, you will work with a simplified spectrogram representation in the form of a random tensor with the shape **(1, 128, 128)**:\n",
        "- The **1** indicates that the audio has a **mono channel**.\n",
        "- **128 √ó 128** corresponds to the **frequency bins** and **time intervals** of the spectrogram.\n",
        "\n",
        "Your implemented model will:\n",
        "1. Take this **stereo spectrogram tensor** as input.\n",
        "2. Encode it into a **latent representation** using an encoder-decoder-like architecture.\n",
        "3. Decode the latent representation to produce a **new noisy spectrogram** as the output.\n",
        "\n",
        "The primary objective is to correctly implement the neural network architecture and its forward pass to achieve the desired functionality.\n",
        "\n"
      ],
      "metadata": {
        "id": "eeb5t4EiSld1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Requirements**\n",
        "\n",
        "---\n",
        "\n",
        "### **Minimum Requirements ‚Äì Sufficient to Pass the Exam**\n",
        "1. **Implement the layers of the architecture:**  \n",
        "   Complete the architecture outlined in Section 1 by filling in the missing parts.\n",
        "2. **Implement the forward function:**  \n",
        "   Ensure the input and output of the forward function are correctly implemented.  \n",
        "   \n",
        "   **Note:** To meet these requirements, your final output must match the expected output provided in **Cell 1.2**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Extra Requirements ‚Äì For Grade Improvement and AI Lab Access**\n",
        "\n",
        "---\n",
        "\n",
        "3. **Fusing embeddings (Preprocessing and Postprocessing Layers):**  \n",
        "   - Add **preprocessing** and **postprocessing** layers to the spectrogram network:  \n",
        "     - **Preprocessing layer**: Load an actual `.wav` audio file and generate its spectrogram using a given method.  \n",
        "     - **Postprocessing layer**: Convert the output spectrogram back into audio format.  \n",
        "   - Integrate these layers into your network architecture to create a complete pipeline from raw audio input to processed audio output.  \n",
        "\n",
        "   ‚û°Ô∏è **Reward: +1 to final grade**\n",
        "\n",
        "---\n",
        "\n",
        "4. **Extending the architecture with a Vision Transformer (ViT) for classification:**  \n",
        "   - Use a pretrained Vision Transformer (ViT) model, such as one available on HuggingFace, to classify spectrograms.  \n",
        "   - Implement this classification for:\n",
        "     - The **original spectrogram** (input to your encoder-decoder network).\n",
        "     - The **final spectrogram** (output from your encoder-decoder network).  \n",
        "   - The classification should determine whether the original and your final spectrograms have been **spoofed** (non-human), with an associated confidence score for each prediction.  \n",
        "\n",
        "   ‚û°Ô∏è **Reward: Access to AI Lab**\n",
        "\n",
        "---\n",
        "\n",
        "Make sure to carefully follow the instructions provided in each cell to meet the requirements!\n"
      ],
      "metadata": {
        "id": "-eJ8a7vIUKLh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#0. Necessary Imports and Data Loading"
      ],
      "metadata": {
        "id": "qFHVdn0GF95L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAofci_7R6kB"
      },
      "outputs": [],
      "source": [
        "# Cell 0.1 (DO NOT EDIT THIS CELL!)\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 0.2 (DO NOT EDIT THIS CELL!)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "WgOK-xATFdDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 0.3 (DO NOT EDIT THIS CELL!)\n",
        "\n",
        "input_spectrogram_tensor = torch.randn(1, 1, 128, 128) # 1 channels for mono audio, 128 frequency bins with 128 frames of audio (with 10 ms hops -> 1280ms -> 1.28 secon audio) (44100Hz sampling rate with hop length of 512 samples gives ~10ms hops)\n",
        "print(input_spectrogram_tensor.shape)"
      ],
      "metadata": {
        "id": "MVP_ivSyOfkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 0.4 (DO NOT EDIT THIS CELL!)\n",
        "\n",
        "spectrogram_plot = input_spectrogram_tensor.squeeze(0).squeeze(0)\n",
        "plt.imshow(spectrogram_plot, cmap='viridis', origin='lower')\n",
        "plt.xlabel(\"Time Frames\")\n",
        "plt.ylabel(\"Frequency Bins\")\n",
        "plt.title(\"Mono Channel Spectrogram\")\n",
        "plt.colorbar(label=\"Intensity\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D2-JWuBmdeJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Architecture**\n",
        "\n",
        "To better view the architecture diagram:  \n",
        "- **Right-click the image** and select **\"Open image in a new tab\"** to enable zoom for a clearer view.  \n",
        "- Alternatively, you can **download the image** using the link below:  \n",
        "  [Download Architecture Diagram](https://drive.google.com/file/d/1Lu6_aamj0npX7tchp3UIhC-Q8abr6Ojj/view?usp=sharing)\n",
        "\n",
        "---\n",
        "\n",
        "### Diagram Preview:\n",
        "![Architecture Diagram](https://drive.google.com/uc?export=view&id=1Lu6_aamj0npX7tchp3UIhC-Q8abr6Ojj)\n"
      ],
      "metadata": {
        "id": "cV5xi065Fk1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1.1 (EDIT THIS CELL!)\n",
        "\n",
        "class SpectrogramEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SpectrogramEncoder, self).__init__()\n",
        "        # Define all layers shown in the provided network diagram.\n",
        "        # Ensure that each layer matches the specifications (e.g., kernel size, stride, padding).\n",
        "        # Pay attention to whether the LSTM expects a specific input shape (e.g., batch_first=True).\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Implement the forward pass based on the given network diagram.\n",
        "        # Use the layers you defined in the __init__ method.\n",
        "\n",
        "        # Hint: Check the shape of `x` after each operation to ensure correctness.\n",
        "        # Example: After Conv2d, the number of channels changes; after MaxPool2d, spatial dimensions shrink.\n",
        "        # Don't forget to reshape the tensor before passing it to the LSTM.\n",
        "\n",
        "        # Pro tip: Use print(f\"Shape after [layer]: {x.shape}\") for debugging.\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        return x\n",
        "\n",
        "\n",
        "class SpectrogramDecoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SpectrogramDecoder, self).__init__()\n",
        "        # Define all layers as per the network diagram.\n",
        "        # Make sure the layer specifications are followed correctly, especially for upsampling or reshaping.\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Implement the forward pass based on the network diagram.\n",
        "\n",
        "        # Hint: Carefully reshape tensors as needed, especially when transitioning between layers.\n",
        "        # Common pitfalls:\n",
        "        # - Mismatched tensor dimensions after reshaping.\n",
        "        # - Incorrect padding or stride causing dimensionality issues.\n",
        "\n",
        "        # Pro tip: Add dummy tensors or placeholders only if specified in the network diagram.\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "hHJ70LQXMVQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1.2 (DO NOT EDIT THIS CELL!)\n",
        "\n",
        "spectrogram_encoder = SpectrogramEncoder()\n",
        "spectrogram_decoder = SpectrogramDecoder()\n",
        "\n",
        "encoded_spectrogram = spectrogram_encoder(input_spectrogram_tensor)\n",
        "decoded_spectrogram = spectrogram_decoder(encoded_spectrogram)\n",
        "\n",
        "print(\"Encoded Spectrogram Shape:\", encoded_spectrogram.shape)\n",
        "print(\"Decoded Spectrogram Shape:\", decoded_spectrogram.shape)\n",
        "\n",
        "try:\n",
        "    assert encoded_spectrogram.shape == (1, 4356, 256), \"Encoded spectrogram shape is incorrect.\"\n",
        "    assert decoded_spectrogram.shape == (1, 1025, 65), \"Decoded spectrogram shape is incorrect.\"\n",
        "    print(\"\\nüéâ Congratulations! Your implementation is correct. You passed the minimum requirement! üéâ\")\n",
        "except AssertionError as e:\n",
        "    print(f\"\\n‚ùå Error: {e}\")\n"
      ],
      "metadata": {
        "id": "y6BWkKwEOgDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. +1 to the Final Grade**\n",
        "\n",
        "In this optional exercise, your task is to download and preprocess the famous Hollywood sound blip (or a similar audio file) and create a complete PyTorch pipeline for spectrogram generation and reconstruction.\n",
        "\n",
        "### **Requirements:**\n",
        "\n",
        "1. **Audio Preprocessing**:\n",
        "   - Convert the raw audio file into a **dual-channel spectrogram** (stereo audio, with left and right channels).\n",
        "   - Reduce the dual-channel spectrogram to a **mono-channel spectrogram**.\n",
        "   - Apply a **logarithmic transformation** to the spectrogram values to compress the dynamic range.\n",
        "   - Interpolate the spectrogram to ensure it has a fixed size of **(1, 128, 128)**, consistent with the random tensor used previously.\n",
        "\n",
        "2. **Postprocessing Module**:\n",
        "   - Create a module to transform the decoder's output tensor back into an audio waveform using the **Griffin-Lim algorithm** for inverse spectrogram transformation.\n",
        "   - The reconstructed waveform should resemble the original audio as closely as possible.\n",
        "\n",
        "3. **Implementation**:\n",
        "   - Build the **preprocessing module** as a PyTorch class that takes a raw audio waveform as input and outputs a processed spectrogram.\n",
        "   - Build the **postprocessing module** as a PyTorch class that takes the spectrogram tensor from the decoder and reconstructs it into an audio waveform.\n",
        "\n",
        "---\n",
        "\n",
        "### **Additional Resources**\n",
        "To better understand the architecture and process:\n",
        "- **Right-click the image** below and select **\"Open image in a new tab\"** for a detailed view.  \n",
        "- Or, **download the architecture diagram** using the link below:  \n",
        "  [Download Architecture Diagram](https://drive.google.com/file/d/1f7akT-yRvOXR89BlkdL7OsWobhLxmCD9/view?usp=sharing)\n",
        "\n",
        "---\n",
        "\n",
        "### **Diagram Preview**:\n",
        "![Architecture Diagram](https://drive.google.com/uc?export=view&id=1f7akT-yRvOXR89BlkdL7OsWobhLxmCD9)"
      ],
      "metadata": {
        "id": "RNJYNiAMYSwQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Download the .wav file and Basic Setup**"
      ],
      "metadata": {
        "id": "JXUxK8slZ2Kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2.1 (DO NOT EDIT THIS CELL!)\n",
        "\n",
        "!wget -qq --no-check-certificate 'https://docs.google.com/uc?export=download&id=1wm0NJ4KgpRUYyZfSPWjrY-HWoRLaTjfs' -O wilhelm_scream.wav"
      ],
      "metadata": {
        "id": "fruPswqWZ4-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2.2 (DO NOT EDIT THIS CELL!)\n",
        "\n",
        "waveform, sample_rate = torchaudio.load(\"wilhelm_scream.wav\")\n",
        "print(f\"Original waveform shape: {waveform.shape}, Sample rate: {sample_rate}\")"
      ],
      "metadata": {
        "id": "ITHu-4fZZ60v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2.3 (DO NOT EDIT THIS CELL!)\n",
        "\n",
        "def plot_spectrograms(*spectrograms, titles = [\"Preprocessed Spectrogram\", \"Postprocessed Spectrogram\"]):\n",
        "    num_spectrograms = len(spectrograms)\n",
        "    plt.figure(figsize=(8 * num_spectrograms, 6))\n",
        "    for i, spectrogram in enumerate(spectrograms):\n",
        "        spectrogram = spectrogram.squeeze().detach().cpu().numpy()\n",
        "        plt.subplot(1, num_spectrograms, i + 1)\n",
        "        plt.imshow(spectrogram, origin='lower', aspect='auto', cmap='viridis')\n",
        "        plt.colorbar(label=\"Intensity\")\n",
        "        plt.title(titles[i])\n",
        "        plt.xlabel(\"Time Frames\")\n",
        "        plt.ylabel(\"Frequency Bins\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "w04rZPkAYU31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Architecture**"
      ],
      "metadata": {
        "id": "DQSN6z7Tfo6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2.4 (EDIT THIS CELL!)\n",
        "\n",
        "class SpectrogramPreprocess(nn.Module):\n",
        "    def __init__(self, sample_rate):\n",
        "        super(SpectrogramPreprocess, self).__init__()\n",
        "        # Initialize a MelSpectrogram transform with the parameters provided in the diagram\n",
        "        # Refer to the 'MelSpectrogram' box in the diagram for values like n_fft, hop_length, and n_mels.\n",
        "        self.mel_spectogram = T.MelSpectrogram(\n",
        "            sample_rate=sample_rate,\n",
        "            n_fft=2048,\n",
        "            hop_length=256,\n",
        "            n_mels=128\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Step 1: Apply MelSpectrogram to convert waveform to spectrogram\n",
        "        # Step 2: Apply log transformation (torch.log1p) to compress dynamic range\n",
        "        # Step 3: Convert stereo to mono using torch.mean\n",
        "        # Step 4: Interpolate to resize the spectrogram (see the diagram for target size and interpolation mode)\n",
        "        # Use the debug tool: print(f\"Shape after [step]: {x.shape}\")\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        return x\n",
        "\n",
        "\n",
        "class SpectrogramPostprocess(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SpectrogramPostprocess, self).__init__()\n",
        "        # Initialize the Griffin-Lim transform\n",
        "        # Refer to the 'GriffinLim' box in the diagram for parameters like n_fft and hop_length.\n",
        "        self.inverse_transform = T.GriffinLim(n_fft=2048, hop_length=256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Step 1: Use GriffinLim to reconstruct the audio waveform from the spectrogram\n",
        "        # Step 2: Save the reconstructed waveform to a WAV file\n",
        "        # Step 3: Reshape the tensor for further processing (refer to the diagram for target shape)\n",
        "        # Use the debug tool: print(f\"Shape after [step]: {x.shape}\")\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        return x\n",
        "\n",
        "\n",
        "class SpectrogramNet(nn.Module):\n",
        "    def __init__(self, sample_rate):\n",
        "        super(SpectrogramNet, self).__init__()\n",
        "        # Define the components: Preprocessor, Encoder, Decoder, Postprocessor\n",
        "        # These should correspond to the boxes in the diagram\n",
        "        # - Preprocessor: Handles input waveform to spectrogram transformation\n",
        "        # - Encoder: Encodes the spectrogram (students should have implemented this in another task)\n",
        "        # - Decoder: Decodes the spectrogram back to its original form\n",
        "        # - Postprocessor: Handles spectrogram-to-waveform transformation and saving\n",
        "        self.preprocessor = SpectrogramPreprocess(sample_rate)\n",
        "        self.encoder = SpectrogramEncoder()\n",
        "        self.decoder = SpectrogramDecoder()\n",
        "        self.postprocessor = SpectrogramPostprocess()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Follow the data flow in the diagram:\n",
        "        # Step 1: Preprocess the waveform to generate a spectrogram\n",
        "        # Step 2: Encode the spectrogram into a compressed representation\n",
        "        # Step 3: Decode the representation back to a spectrogram\n",
        "        # Step 4: Postprocess to reconstruct the waveform and save it\n",
        "        # Optional: Plot the spectrograms at the preprocessing and postprocessing stages\n",
        "\n",
        "        # Use the debug tool: print(f\"Shape after [component]: {x.shape}\")\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "eiyOZh04a4Pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2.5 (DO NOT EDIT THIS CELL!)\n",
        "\n",
        "full_spectrogram_net = SpectrogramNet(sample_rate = sample_rate)\n",
        "spectrogram_preprocessed, spectrogram_postprocessed = full_spectrogram_net(waveform)\n",
        "\n",
        "print(\"Preprocessed Spectrogram Shape:\", spectrogram_preprocessed.shape)\n",
        "print(\"Postprocessed Spectrogram Shape:\", spectrogram_postprocessed.shape)\n",
        "\n",
        "try:\n",
        "    assert spectrogram_preprocessed.shape == (1, 1, 128, 128), \"Preprocessed spectrogram shape is incorrect.\"\n",
        "    assert spectrogram_postprocessed.shape == (1, 1, 128, 128), \"Postprocessed spectrogram shape is incorrect.\"\n",
        "    assert os.path.isfile(\"output_audio.wav\"), \"'output_audio.wav' file is not found in the current working directory.\"\n",
        "    print(\"\\nüéâ Congratulations! Your implementation is correct. You increased your final grade by 1! üéâ\")\n",
        "except AssertionError as e:\n",
        "    print(f\"\\n‚ùå Error: {e}\")"
      ],
      "metadata": {
        "id": "DHxgEQQifzVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Access to AI Lab**\n",
        "\n",
        "In this task, you are required to classify both the **preprocessed** and **postprocessed** spectrograms into two classes:\n",
        "- **1 - Spoofed (Non-Human)**  \n",
        "- **0 - Real (Human)**  \n",
        "\n",
        "### **Instructions**\n",
        "1. **Model Information**:  \n",
        "   You will use the following Vision Transformer (ViT) model from **HuggingFace**: [Model Link](https://huggingface.co/MattyB95/VIT-ASVspoof2019-Mel_Spectrogram-Synthetic-Voice-Detection)  \n",
        "\n",
        "   This model is fine-tuned on `google/vit-base-patch16-224-in21k` to classify spectrograms as either **spoofed** or **not spoofed**. It requires an **RGB image** of the spectrogram as input.\n",
        "\n",
        "2. **Preprocessing the Input**:  \n",
        "   - Transform the preprocessed and postprocessed spectrogram tensors into **3-channel RGB images** using a suitable colormap of your choice (e.g., `viridis`, `plasma`, etc.).\n",
        "   - Ensure the transformed images are compatible with the model's input size and format.\n",
        "\n",
        "3. **Classifying the Spectrograms**:  \n",
        "   - Load the pretrained Vision Transformer model from HuggingFace.\n",
        "   - Use the model to classify both the preprocessed and postprocessed spectrograms.\n",
        "   - Extract the **logits** from the model's output and convert them to **class probabilities**.\n",
        "\n",
        "4. **Visualizing the Results**:  \n",
        "   - Clearly label the confidence scores and indicate whether the spectrogram is classified as **spoofed (1)** or **real (0)**.\n",
        "   - Plot the confidence scores with the correct label.\n",
        "\n",
        "---\n",
        "\n",
        "### **Additional Resources**\n",
        "A possible solution pipeline might look like the following:\n",
        "- **Right-click the image** below and select **\"Open image in a new tab\"** for a detailed view.  \n",
        "- Or, **download the architecture diagram** using the link below:  \n",
        "  [Download Architecture Diagram](https://drive.google.com/file/d/1zd31dK9OXkD_EA5_C1W6uDorKGd3SvP6/view?usp=sharing)\n",
        "\n",
        "---\n",
        "\n",
        "### **Diagram Preview**:\n",
        "![Architecture Diagram](https://drive.google.com/uc?export=view&id=1zd31dK9OXkD_EA5_C1W6uDorKGd3SvP6)\n",
        "\n",
        "Good luck!\n"
      ],
      "metadata": {
        "id": "637zP7iiaagl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3.1 (EDIT THIS CELL!)\n",
        "\n",
        "from matplotlib.colors import Normalize\n",
        "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
        "\n",
        "preprocessed_spectrogram_label = None\n",
        "postprocessed_spectrogram_label = None\n",
        "\n",
        "# Steps:\n",
        "# 1. Convert spectrogram tensors to NumPy arrays.\n",
        "# 2. Normalize the spectrogram using `matplotlib.colors.Normalize`.\n",
        "# 3. Apply colormap ('viridis') and convert back to tensors for input into the ViT model.\n",
        "# 4. Use `AutoImageProcessor` and `AutoModelForImageClassification` for classification.\n",
        "# 5. Perform inference and extract labels (0 = Real, 1 = Spoofed).\n",
        "\n",
        "# Hints:\n",
        "# - Use `.permute()` to rearrange tensor dimensions to match ViT input expectations.\n",
        "# - Use `torch.nn.functional.interpolate` for resizing.\n",
        "# - Make use of `.softmax(dim=1)` on logits to get probabilities before extracting labels with `.argmax(dim=1)`.\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "Iqw28sAcmNy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3.2 (DO NOT EDIT THIS CELL!)\n",
        "\n",
        "try:\n",
        "    assert preprocessed_spectrogram_label == 1, \"Preprocessed spectrogram label is incorrect.\"\n",
        "    assert postprocessed_spectrogram_label == 1, \"Postprocessed spectrogram label is incorrect.\"\n",
        "    print(\"\\nüéâ Congratulations! Your implementation is correct. You got access to the AI Lab! üéâ\")\n",
        "except AssertionError as e:\n",
        "    print(f\"\\n‚ùå Error: {e}\")"
      ],
      "metadata": {
        "id": "mVpXZ0p2anFt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}